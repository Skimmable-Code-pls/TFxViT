### Tensorflow notes
- [Keras Self-Attention](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/attention/attention.py#L12-L15) is only for 1 Head.
- [Tensorflow Multi-Head Self-Attention in nn_blocks.py](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L1627-1658) = [Multi-Head Self-Attention](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_layers.py#L1332-1335) with optional choice to pick Multi-Query Attention instead of Multi-Head Self-Attention. Multi-Query Attention comes with 2 options [with](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L1267) or [without Downsampling](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L1197). Practically, [Transformer block in nn_blocks.py only exclusively uses Multi-Head Self-Attention](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L2605-2622) so no point including Multi-Query Attention in my implementation.

### Timm notes
-  [fused_attn in Timm](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L59) only checks if F.scaled_dot_product_attn is available in your torch and whether your GPU is in macOS is only checking if F.scaled_dot_product_attn is available in your downloaded Pytorch version and whether your GPU is in macOS [see here](https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/config.py#L137-L143). Pytorch has [F.scaled_dot_product_attn source code in C](https://github.com/pytorch/pytorch/blob/51c6c5e156c64d84ff0cd06a559fa6786c96f128/aten/src/ATen/native/transformers/attention.cpp#L692-752) that I can't find anywhere in Tensorflow. Despite being written in C, there's a Python version on documentation. My Jupyter Notebook verifies that [forward pass in  F.scaled_dot_product_attention's equivalent in python](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention) and [forward pass in Timm Multi-Head Self-Attention](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L98-L102) are not only look identical but produce identical result despite large input tensor. So we can ignore F.scaled_dot_product_attn and use the manual Timm and translate to Tensorflow version.
