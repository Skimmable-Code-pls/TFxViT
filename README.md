### Road map
- [ ] Scaled Dot Product Attention with customisable scaling. Steal from [torch F.scaled-dot-product-attention semi-source code](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention). Then benchmark tensor similarity, weight importable, and attention map.
- [ ] Self-Attention. Steal from [Timm Multi-Heads Attention](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L58) but take considerations in design philosophy of [Tensorflow Multi-Heads Attention](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_layers.py#L1286) wherever possible. Benchmark like above between my implementations against official versions in Timm and Tensorflow, respectively.
- [ ] ViT. Steal from [Timm Vision Transformer](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py). Benchmark my implementations against official versions in Timm and Tensorflow, respectively.
- [ ] Swin. Steal from [Timm Swin](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/swin_transformer.py). Benchmark like above between my implementations against official versions in Timm and Tensorflow, respectively.
- [ ] RSBuilding. That model is built from backbone [Vision Transformer SAM from Open-CD](https://github.com/Meize0729/RSBuilding/blob/main/opencd/models/backbones/vit_sam_normal.py). From first impression, it looks like [Timm Vision Transformer SAM](https://github.com/Meize0729/RSBuilding/blob/main/opencd/models/backbones/vit_sam_normal.py) with couple extra functions. Benchmark like above between my implementations against official versions in Timm and Tensorflow, respectively.

