### Road map
At every step, benchmark my Tensorflow implementation against Timm and official Tensorflow's implementation (if available). For every 3 months, scroll down to the bottom [Timm benchmark](https://github.com/huggingface/pytorch-image-models/blob/main/results/benchmark-infer-fp32-nchw-pt221-cpu-i9_10940x-dynamo.csv) to lookout for modern backbone.
- [ ] Self-Attention. Steal from [Timm Multi-Heads Attention](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L58) but take considerations in design philosophy of [Tensorflow Multi-Heads Attention](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_layers.py#L1286) wherever possible so that it fits model.compile in Tensorflow.
- [ ] ViT. Steal from [Timm Vision Transformer](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py). Benchmark my implementations against official versions in Timm and Tensorflow, respectively.
- [ ] Swin. Steal from [Timm Swin](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/swin_transformer.py).
- [ ] SAM. Steal from [Timm SAM](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer_sam.py#L736).

Experimental models:
- [ ] SwinV2. Steal from [Timm SwinV2](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/swin_transformer_v2_cr.py). To clarify, Timm swin_transformer_v2_cr.py more closely follows [original author's code](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer_v2.py) than Timm swin_transformer_v2.py. That said, follow this code with caution for many FIXME issues to be fixed: The answer for [1st FIXME](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/swin_transformer_v2_cr.py#L114) is importing embed_dim from one of those [yaml model config](https://github.com/microsoft/Swin-Transformer/blob/main/configs/swinv2). I'm not sure if [3rd FIXME](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/swin_transformer_v2_cr.py#L371-L374) is still an issue because it looks similar to [original version](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer_v2.py#L280). In additions, [don't use "before padding" as pretrained SwinV2 wasn't trained with that on](https://github.com/huggingface/pytorch-image-models/issues/2438#issuecomment-2651749230).
- [ ] SAM2. Steal from [Timm SAM2]([https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer_sam.py#L736](https://github.com/rwightman/timm/blob/main/timm/models/hieradet_sam2.py)).

Domain-specific experimental models:
- [ ] RSBuilding. That model is built from backbone [Vision Transformer SAM from Open-CD](https://github.com/Meize0729/RSBuilding/blob/main/opencd/models/backbones/vit_sam_normal.py). From first impression, it looks like [Timm Vision Transformer SAM](https://github.com/Meize0729/RSBuilding/blob/main/opencd/models/backbones/vit_sam_normal.py) with couple extra functions. 
