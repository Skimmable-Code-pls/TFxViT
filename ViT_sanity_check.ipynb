{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Jsb2qxXoBlEW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from timm.layers import trunc_normal_\n",
        "from typing import Type\n",
        "\n",
        "### model config for vit_large_patch16_224 (search Google)\n",
        "dtype = torch.float32\n",
        "embed_dim = 1024\n",
        "qkv_bias = True\n",
        "num_heads=16\n",
        "norm_layer: Type[nn.Module] = nn.LayerNorm\n",
        "qk_norm = True\n",
        "dropout_p = 0.0\n",
        "\n",
        "# if use input minibatch shape (16, 3, 224, 224) for vit_large_patch16_224, then x tensor shape would be\n",
        "x = torch.randn((16, 196, embed_dim), dtype=dtype)\n",
        "\n",
        "# initialised vars from model config:\n",
        "head_dim = embed_dim // num_heads\n",
        "scale = head_dim ** -0.5\n",
        "qkv = nn.Linear(embed_dim, embed_dim * 3, bias=qkv_bias)\n",
        "q_norm = norm_layer(head_dim) if qk_norm else nn.Identity()\n",
        "k_norm = norm_layer(head_dim) if qk_norm else nn.Identity()\n",
        "\n",
        "# Steal from Timm: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L858-L865\n",
        "def init_weights_vit_timm(module: nn.Module, name: str = '') -> None:\n",
        "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
        "    if isinstance(module, nn.Linear):\n",
        "        trunc_normal_(module.weight, std=.02)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif hasattr(module, 'init_weights'):\n",
        "        module.init_weights()\n",
        "init_weights_vit_timm(qkv)\n",
        "\n",
        "# Creating Q, K, V\n",
        "batch_size, num_seq, channels = x.shape\n",
        "qkv = qkv(x).reshape(batch_size, num_seq, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
        "q, k, v = qkv.unbind(0)\n",
        "q, k = q_norm(q), k_norm(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minor test: Check similarity between @ and matmul"
      ],
      "metadata": {
        "id": "4LEJnn8a4wK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_logit_1 = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "attn_logit_2 = q @ k.transpose(-2, -1) * scale\n",
        "print(\"L2-norm for 1st and 2nd Attention Logits\", torch.norm(attn_logit_1.detach() - attn_logit_2.detach(), p=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KinjjADdBuC3",
        "outputId": "2075c82d-484a-4884-cfe3-7fb500daa648"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-norm for 1st and 2nd Attention Logits tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Timm Multi-head Self-Attention vs F.scaled_dot_product_attention\n",
        "\n",
        "- Attention Logits is right before you apply softmax"
      ],
      "metadata": {
        "id": "NAoHoLPcEoco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### F.scaled_dot_product_attention\n",
        "L, S = q.size(-2), k.size(-2)\n",
        "attn_bias = torch.zeros(L, S, dtype=q.dtype)\n",
        "\n",
        "attn_logit_F = q @ k.transpose(-2, -1) * scale\n",
        "attn_logit_F += attn_bias\n",
        "attn_score_F = torch.softmax(attn_logit_F, dim=-1)\n",
        "attn_score_F = torch.dropout(attn_score_F, dropout_p, train=True)\n",
        "output_F = attn_score_F @ v\n",
        "\n",
        "### Timm Multi-Head Self-Attention\n",
        "attn_drop = nn.Dropout(dropout_p)\n",
        "\n",
        "q = q * scale\n",
        "attn_logit_Timm = q @ k.transpose(-2, -1)\n",
        "attn_score_Timm = attn_logit_Timm.softmax(dim=-1)\n",
        "attn_score_Timm = attn_drop(attn_score_Timm)\n",
        "output_Timm = attn_score_Timm @ v\n",
        "\n",
        "# Linear algebra says this should be identical due to Associative rule but let's check\n",
        "print(\"L2 for attn_logit_F and attn_logit_Timm\", torch.norm(attn_logit_F.detach() - attn_logit_Timm.detach(), p=2))\n",
        "print(\"L2 for attn_score_F and attn_score_Timm\", torch.norm(attn_score_F.detach() - attn_score_Timm.detach(), p=2))\n",
        "print(\"L2 for output_F and output_Timm\", torch.norm(output_F.detach() - output_Timm.detach(), p=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2mtTZgYBwn0",
        "outputId": "ab083253-3209-4182-8713-6a2f58e412fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 for attn_logit_F and attn_logit_Timm tensor(0.)\n",
            "L2 for attn_score_F and attn_score_Timm tensor(0.)\n",
            "L2 for output_F and output_Timm tensor(0.)\n"
          ]
        }
      ]
    }
  ]
}