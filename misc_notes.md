### Tensorflow Multi-Head Self-Attention coding summary
- [Keras Self-Attention](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/attention/attention.py#L12-L15) is only for 1 Head.
- [Tensorflow Multi-Head Self-Attention in nn_blocks.py](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L1627-1658) = [Multi-Head Self-Attention](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_layers.py#L1332-1335) with optional choice to pick Multi-Query Attention instead of Multi-Head Self-Attention. Multi-Query Attention comes with 2 options [with](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L1267) or [without Downsampling](https://github.com/tensorflow/models/blob/v2.18.0/official/vision/modeling/layers/nn_blocks.py#L1197).
